version: 2.1

orbs: 
  aws-cli: circleci/aws-cli@2.0.3

commands:
  # Exercise: Reusable Job Code
  print_pipeline_id:
    parameters:
      id: 
        type: string
    steps:
      - run: echo << parameters.id >>

  # Exercise - Rollback
  destroy_environment:
    steps:
      - run:
          name: Destroy environment
          # ${CIRCLE_WORKFLOW_ID} is a Built-in environment variable 
          # ${CIRCLE_WORKFLOW_ID:0:5} takes the first 5 chars of the variable CIRCLE_CI_WORKFLOW_ID 
          when: on_fail
          command: |
            aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:7}

jobs:

  s3Bucket:  # Choose any name, such as `build`
    # The primary container, where your job's commands will run
    docker:
      - image: amazon/aws-cli 
    steps:
      - checkout # check out the code in the project directory
      - run: 
          name: "Check bucket" # run the `echo` command
          command: aws s3 ls
  
  create_infrastructure: 
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
              --template-file templates.yml \
              --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:7} \
              --region us-east-1

  configure_infrastructure: 
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - add_ssh_keys:
              # You can get this ID in the section where you registered the SSH Key
              fingerprints: ["2e:a6:38:52:2d:ad:48:27:44:1a:22:00:93:43:32:5d"]
      - run:
          name: Install Ansible
          command: |
            # Install Ansible
            apk add --update ansible
      - run:
          name: Run Playbook and Configure server
          command: |
            # Your command
            ansible-playbook -i inventory.txt main2.yml
  
  # Exercise: Smoke Testing
  # smoke_test:
  #   docker:
  #     - image: alpine:latest
  #   steps:
  #     - run: apk add --update curl
  #     - run:
  #         name: smoke test
  #         command: |
  #           URL="https://blog.udacity.com/"
  #           # Test if website exists
  #           if curl -s --head ${URL} 
  #           then
  #             return 0
  #           else
  #             return 1
  #           fi

  # Exercise: Smoke Testing
  smoke_test:
    docker:
      - image: alpine:latest
    steps:
      - run: apk add --update curl
      - run:
          name: smoke test
          command: |
            URL="https://blog.udacity333.com/"
            # Test if website exists
            if curl -s --head ${URL} 
            then
              return 0
            else
              return 1
            fi
      - destroy_environment

  # Job 1
  # Executes the bucket.yml - Deploy an S3 bucket, and interface with that bucket to synchronize the files between local and the bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the bucket.yml template file.
  create_and_deploy_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute bucket.yml - Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
            --template-file bucket.yml \
            --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7} \
            --parameter-overrides MyBucketName="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
      # Uncomment the step below if yoou wish to upload all contents of the current directory to the S3 bucket
      # - run: aws s3 sync . s3://mybucket-${CIRCLE_WORKFLOW_ID:0:7} --delete
  
  # Exercise: Promote to Production - Job 2
  # Fetch and save the pipeline ID (bucket ID) responsible for the last release.
  get_last_deployment_id:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - run:
          name: Fetch and save the old pipeline ID (bucket name) responsible for the last release.
          command: |
            aws cloudformation \
            list-exports --query "Exports[?Name==\`PipelineID\`].Value" \
            --no-paginate --output text > ~/textfile.txt
      - persist_to_workspace:
          root: ~/
          paths: 
            - textfile.txt 
  
  # Exercise: Promote to Production - Job 3
  # Executes the cloudfront.yml template that will modify the existing CloudFront Distribution, change its target from the old bucket to the new bucket - `mybucket-${CIRCLE_WORKFLOW_ID:0:7}`. 
  # Notice here we use the stack name `production-distro` which is the same name we used while deploying to the S3 bucket manually.
  promote_to_production:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute cloudfront.yml
          command: |
            aws cloudformation deploy \
            --template-file cloudfront.yml \
            --stack-name production-distro \
            --parameter-overrides PipelineID="mybucket-${CIRCLE_WORKFLOW_ID:0:7}" 
  
   # Exercise: Promote to Production - Job 4
  # Destroy the previous production version's S3 bucket and CloudFormation stack. 
  clean_up_old_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - attach_workspace:
          at: ~/
      - run:
          name: Destroy the previous production version's S3 bucket and CloudFormation stack. 
          # Use $OldBucketID environment variable or mybucket644752792305 below.
          # Similarly, you can create and use $OldStackID environment variable in place of production-distro 
          command: |
            export OldBucketID=$(cat ~/textfile.txt)
            aws s3 rm "s3://${OldBucketID}" --recursive
          #  aws cloudformation delete-stack --stack-name production-distro 
          #  aws cloudformation delete-stack --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7}
          #  aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5}

# Sequential workflow
workflows:
  # Name the workflow
  workflow:
    jobs:
      - s3Bucket
      # - create_infrastructure
      # - configure_infrastructure
      # - smoke_test
      # - create_and_deploy_front_end
      # - get_last_deployment_id
      # - promote_to_production
      - clean_up_old_front_end

# # check for errors
# jobs:
#   my_job:
#     docker:
#       - image: circleci/node:13.8.0
#     steps:
#       - run: exit 1
#       - run:
#           name: on error
#           command: echo "Hello Error!"
#           when: on_fail

# workflows:
#   my_workflow:
#     jobs:
#       - my_job

#Upload and copy files
# steps:
#   - run:
#       name: Testing application
#       command: make test
#       shell: /bin/bash
#       working_directory: ~/my-app
#       no_output_timeout: 30m
#       environment:
#         FOO: bar

#   - run: echo 127.0.0.1 devhost | sudo tee -a /etc/hosts

#   - run: |
#       sudo -u root createuser -h localhost --superuser ubuntu &&
#       sudo createdb -h localhost test_db

#   - run:
#       name: Upload Failed Tests
#       command: curl --data fail_tests.log http://example.com/error_logs
#       when: on_fail

# To create reusable code
# version: 2.1

# commands:
#   print_pipeline_id:
#     steps:
#       - run: echo ${CIRCLE_WORKFLOW_ID}

# jobs:
#   my_job:
#     docker:
#       - image: circleci/node:13.8.0
#     steps:
#       - print_pipeline_id

# workflows:
#   my_workflow:
#     jobs:
#       - my_job
